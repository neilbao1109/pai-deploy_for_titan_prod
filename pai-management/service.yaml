# Copyright (c) Microsoft Corporation
# All rights reserved.
#
# MIT License
#
# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
# documentation files (the "Software"), to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and
# to permit persons to whom the Software is furnished to do so, subject to the following conditions:
# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING
# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,
# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

imagelist:
  # If your service have custom image, please set the detail here.
  # The image name should be the same as the folder in src.
  # please set your template file name in the templatelist. If there is no template in this image, fill None.
  # If the docker image have prerequisite custom image, please fill it in  prerequisite. If no, fill None.
  base-image:
    templatelist:
      - None
    copy:
      - host-configure/
    prerequisite: None

  drivers:
    templatelist:
      - dockerfile
    prerequisite: None

  hadoop-run:
    templatelist:
      # dockerfile.template
      - dockerfile
    prerequisite: base-image

  zookeeper:
    templatelist:
      - dockerfile
    prerequisite: base-image

  frameworklauncher:
    # the copy will be placed in the path src/framworklauncher/copied_file
    copy:
      - ../frameworklauncher
    templatelist:
      - dockerfile
    prerequisite: hadoop-run

  rest-server:
    copy:
      # created by the prepare hadoop function on docker_build.py
      - src/hadoop-run/hadoop
      - ../rest-server
    templatelist:
      - dockerfile
    prerequisite: None

  webportal:
    copy:
      - ../pai-fs
      - ../job-tutorial
      - ../webportal
    templatelist:
      - None
    prerequisite: None

  cleaning-image:
    templatelist:
      - dockerfile
    prerequisite: None
 
  gpu-exporter:
    copy:
      - ../prometheus/exporter
    templatelist:
      - None
    prerequisite: None
    
  grafana:
    copy:
      - ../grafana/dashboards
    templatelist:
      - None
    prerequisite: None
    
  pylon:
    copy:
      - ../pylon
    templatelist:
      - None
    prerequisite: None

  end-to-end-test:
    templatelist:
      - dockerfile
    prerequisite: hadoop-run

# Before starting your service, ensure the kubectl has been installed on your host.
# If you want to remove a certain service, comment it in the servicelist.
servicelist:
  cleaning:
    prerequisite:
      - None
    templatelist:
      - cleaning.yaml

  cluster-configuration:
    prerequisite:
      - None
    templatelist:
      - secret.yaml
      - host-configuration/host-configuration.yaml
      - gpu-configuration/gpu-configuration.yml
      - gpu-configuration/gpu-configuration.json
      - docker-credentials/config.json
      - cleanup.sh
    startscript: start.sh
    stopscript: cleanup.sh

  drivers:
    prerequisite:
      - cluster-configuration
    templatelist:
      - node-label.sh
      - drivers.yaml
      - cleanup.sh
    # Noteï¼š your script should start all your service dependency. Make sure service has completed the starting process.
    startscript: start.sh
    stopscript: cleanup.sh

  hadoop-service:
    prerequisite:
      - cluster-configuration
      - drivers
    templatelist:
      - node-label.sh
      - configmap-create.sh
      - hadoop-name-node.yaml
      - hadoop-data-node.yaml
      - hadoop-resource-manager.yaml
      - hadoop-node-manager.yaml
      - hadoop-jobhistory.yaml
      - zookeeper.yaml
      - one-time-job-hadoop.yaml
      - cleanup.sh
      # Step 3 of 4 to set up Hadoop queues.
      # Tell deploy.py to instantiate capacity-scheduler.xml.content from
      # capacity-scheduler.xml.content.template.
      - hadoop-configuration/capacity-scheduler.xml.content
    startscript: start.sh
    stopscript: cleanup.sh

  frameworklauncher:
    prerequisite:
      - cluster-configuration
      - hadoop-service
    templatelist:
      - node-label.sh
      - frameworklauncher.yaml
      - cleanup.sh
    startscript: start.sh
    stopscript: cleanup.sh

  etcd:
    prerequisite:
      - None
    templatelist:
      - node-label.sh
      - etcd.yaml
      - cleanup.sh
    startscript: start.sh
    stopscript: cleanup.sh

  rest-server:
    prerequisite:
      - etcd
      - cluster-configuration
      - frameworklauncher
    templatelist:
      - node-label.sh
      - rest-server.yaml
      - cleanup.sh
    startscript: start.sh
    stopscript: cleanup.sh

  webportal:
    prerequisite:
      - cluster-configuration
      - rest-server
    templatelist:
      - node-label.sh
      - webportal.yaml
      - cleanup.sh
    startscript: start.sh
    stopscript: cleanup.sh

  prometheus:
    prerequisite:
      - cluster-configuration
      - drivers
    templatelist:
      - node-label.sh
      - node-exporter-ds.yaml
      - prometheus-configmap.yaml
      - prometheus-deployment.yaml
      - cleanup.sh
    startscript: start.sh
    stopscript: cleanup.sh

  grafana:
    prerequisite:
      - cluster-configuration
      - prometheus
    templatelist:
      - node-label.sh
      - grafana.yaml
      - cleanup.sh
      - grafana-configuration/pai-clusterview-dashboard.json
      - grafana-configuration/pai-nodeview-dashboard.json
      - grafana-configuration/pai-jobview-dashboard.json
      - grafana-configuration/pai-taskroleview-dashboard.json
      - grafana-configuration/pai-taskview-dashboard.json
      - grafana-configuration/prom-datasource.json
    startscript: start.sh
    stopscript: cleanup.sh

  pylon:
    prerequisite:
      - cluster-configuration
    templatelist:
      - node-label.sh
      - pylon.yaml
      - cleanup.sh
    startscript: start.sh
    stopscript: cleanup.sh

  end-to-end-test:
    # The end-to-end test will run in the cluster by default to
    # make sure PAI is built and deployed correctly and working normally.
    # Comment this block to cancel end-to-end test or stop
    # end-to-end-test-deployment pod to terminate the test.
    prerequisite:
      - cluster-configuration
      - frameworklauncher
      - rest-server
    templatelist:
      - end-to-end-test.yaml
      - cleanup.sh
    startscript: start.sh
    stopscript: cleanup.sh

  spark:
    prerequisite:
      - hadoop-service
    templatelist:
      - node-label.sh
      - spark-base.yaml
      - cleanup.sh
    startscript: start.sh
    stopscript: cleanup.sh

#  seldon:
#    prerequisite:
#      - None
#    templatelist:
#      - None
#    startscript: start.sh
#    stopscript: cleanup.sh
